{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bittflowvirtualenvwrapperff4a3ff39ecc4e298fd2c21fd802ff24",
   "display_name": "Python 3.8.5 64-bit ('tflow': virtualenvwrapper)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Playing with Horse-Human data set from Laurance Moroney\n",
    "\n",
    "In this notebook we are going to play with complex image classification. Previously, we played with MNIST, CIFAR and found that those data sets were still very simple and very much specialized to their data set. We have learnt a great tool (CNN) that can do feature extraction for complex computer vision and hence we wish to explore more. We will play with a dataset compiled by Laurance Moroney that contains two image classes: Horses and Humans.\n",
    "\n",
    "## HorseHumanSmall dataset\n",
    "Dataset contains different complex images showing horses and humans. These are graphics basically, not real humans or horses. But the scenes are detailed. Like there are backgrounds, shades, different poses, zoom, and varying grid positions etc.\n",
    "Here is how you get the data set:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
    "    -O /tmp/horse-or-human.zip\n",
    "\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n",
    "    -O /tmp/validation-horse-or-human.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "localZip = '/tmp/horse-or-human.zip'\n",
    "zipRef = zipfile.ZipFile(localZip)\n",
    "zipRef.extractall('/tmp/horse-or-human/')\n",
    "localZip = '/tmp/validation-horse-or-human.zip'\n",
    "zipRef = zipfile.ZipFile(localZip)\n",
    "zipRef.extractall('/tmp/validation-horse-or-human')\n",
    "zipRef.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo 'Training Humans:' && ls /tmp/horse-or-human/humans | wc -l\n",
    "!echo 'Training Horses:' && ls /tmp/horse-or-human/horses | wc -l\n",
    "!echo 'Validation Humans:' && ls /tmp/validation-horse-or-human/humans | wc -l\n",
    "!echo 'Training Horses:' && ls /tmp/validation-horse-or-human/horses | wc -l"
   ]
  },
  {
   "source": [
    "There are 527 (not sure why 27 more human images, probably an honest mistake) training images for humans and 500 training images for horses. There are 128 images each for validation. This turns out to be quite a small dataset. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDatasetDir = os.path.join('/tmp/horse-or-human/')\n",
    "valDatasetDir = os.path.join('/tmp/validation-horse-or-human/')\n",
    "trainImgsHumans = os.path.join(trainDatasetDir, '/humans')\n",
    "trainImgsHorses = os.path.join(trainDatasetDir, '/horses')\n",
    "valImgsHumans = os.path.join(valDatasetDir, '/humans')\n",
    "valImgsHorses = os.path.join(valDatasetDir, '/horses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300,300,3)),\n",
    "        tf.keras.layers.MaxPooling2D((2,2)),\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2,2)),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2,2)),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2,2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "source": [
    "## Loading custom image datasets\n",
    "There are loads of datasets available directly from tensorflow (TFDS) which are loadable basically in one line. If we wish to load our own datasets, we would need some help from tesnorflow. For images, tensorflow provides ImageGenerator APIs that can be used load custom image datasets into a format tensorflow understands. \n",
    "\n",
    "### Augemntation\n",
    "One important concept to grasp in ImageGenerator API is augmentation. Augmentation helps against overfitting by augmenting the dataset with some nifty tricks. There are many augmentation options https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator.\n",
    "\n",
    "#### rescale = <Normalize> \n",
    "For example if pixel values were 0-255, normalize them from 0-1.\n",
    "\n",
    "#### rotation_range = <Angle of rtoation> \n",
    "Think if all the cat images in youtr dataset were sitting upright with their ears pointing up. You might overfit your model with cats only having ears upright. By using rotation_angle you basically tell tensorflow to rotate your random image by a given angle so that you avoid overfitting and being versatility to your model.\n",
    "\n",
    "#### width_shift_range = <change position of artifact width wise>\n",
    "Think if all the images in your dataset were of persons standing in the middle of the image. This wil result in overfitting as the model may not recognize persons standing on the left or right. width_shift_range augments the image by shifting the position width wise by a given factor. Values between 0-1 are used where 1 is 100% and 0 is 0%, i.e, 0.20 would mean 20%\n",
    "\n",
    "#### height_shift_range = <change position of the artifact height wise>\n",
    "Same analogy as above. Shift is height wise.\n",
    "\n",
    "#### shear_range = <shears the image>\n",
    "Think of a person lying down on the ground. He/she is clearly a human, but if our training data contained only images of persons standing up only, the model may fail to recognize. shear_range tilts the image in all axes so that we would get somthing very close to an image of a person lying down. 0.20 would mean 20% skew.\n",
    "\n",
    "#### zoom_range = <zoom in>\n",
    "Imagine our dataset had images of people with full body shown and if the model trained with this data was fed an image with a person's image zoomed in and now his/her legs are missing from the exposure then the model may exhibit overfitting. zoom_range tackles this by randomly zooming in an image so that this feature is also accounted for.\n",
    "\n",
    "#### horizontal_flip = <flip the image>\n",
    "Think if all the images in the dataset are the persons with their right hand raised up, and the model trained with this data was fed an image with a left hand raised up. horizontal_flip, randomly flips an image to capture this feature.\n",
    "\n",
    "#### fill_mode = <fill wiped out pixels>\n",
    "Some of the augmentation options, e.g., shear_range, shift_range etc are going to wipe out pixels while applying transformations. fill_mode, helps to fill in the gaps left after transformations. One of the fill modes is 'nearest' which means fill the wiped out pixels with the nearest neighbouring pixels.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaleVal = 1./255\n",
    "print(rescaleVal)\n",
    "trainDataGen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=rescaleVal,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.20,\n",
    "    height_shift_range=0.20,\n",
    "    horizontal_flip=True,\n",
    "    #vertical_flip=True,\n",
    "    zoom_range=0.20,\n",
    "    shear_range=0.20,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "trainingGenerator = trainDataGen.flow_from_directory(\n",
    "    trainDatasetDir,\n",
    "    target_size=(300,300),\n",
    "    class_mode='binary',\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "valDataGen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=rescaleVal\n",
    ")\n",
    "\n",
    "validationGenerator = valDataGen.flow_from_directory(\n",
    "    valDatasetDir,\n",
    "    target_size=(300,300),\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    trainingGenerator, \n",
    "    steps_per_epoch=8, \n",
    "    epochs=15, \n",
    "    validation_data=validationGenerator, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}